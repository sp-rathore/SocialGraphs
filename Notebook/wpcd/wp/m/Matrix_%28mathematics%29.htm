<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html dir="ltr" lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
  <meta content="Matrix (mathematics),Matrix (mathematics),1693,1750,1848,Aeroelasticity,Arthur Cayley,Associative algebra,Basis (linear algebra),Carl Friedrich Gauss,Characteristic polynomial" name="keywords" />
  <link href="../../favicon.ico" rel="shortcut icon" />
  <link href="../../wp/w/Wikipedia_Text_of_the_GNU_Free_Documentation_License.htm" rel="copyright" />
  <title>Matrix (mathematics)</title>
  <style media="screen,projection" type="text/css">/*<![CDATA[*/ @import "../../css/wp-monobook-main.css"; /*]]>*/</style>
  <link href="../../css/wp-commonPrint.css" media="print" rel="stylesheet" type="text/css" />
  <!--[if lt IE 5.5000]><style type="text/css">@import "../../css/IE50Fixes.css";</style><![endif]-->
  <!--[if IE 5.5000]><style type="text/css">@import "../../css/IE55Fixes.css";</style><![endif]-->
  <!--[if IE 6]><style type="text/css">@import "../../css/IE60Fixes.css";</style><![endif]-->
  <!--[if IE 7]><style type="text/css">@import "../../css/IE70Fixes.css";</style><![endif]-->
  <!--[if lt IE 7]><script type="text/javascript" src="../../js/IEFixes.js"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->
  <script type="text/javascript">
			var skin = "monobook";
			var stylepath = "/skins-1.5";

			var wgArticlePath = "/wiki/$1";
			var wgScriptPath = "/w";
			var wgServer = "http://en.wikipedia.org";

			var wgCanonicalNamespace = "";
			var wgNamespaceNumber = 0;
			var wgPageName = "Matrix_(mathematics)";
			var wgTitle = "Matrix (mathematics)";
			var wgArticleId = 124914;
			var wgCurRevisionId = 92851040;
			var wgIsArticle = true;
		
			var wgUserName = null;
			var wgUserLanguage = "en";
			var wgContentLanguage = "en";
		</script>
  <script src="../../js/wikibits.js" type="text/javascript"><!-- wikibits js --></script>
  <script src="../../js/wp.js" type="text/javascript"><!-- site js --></script>
  <style type="text/css">/*<![CDATA[*/
@import "../../css/wp-common.css";
@import "../../css/wp-monobook.css";
@import "../../css/wp.css";
/*]]>*/</style>
  <!-- Head Scripts -->
 </head>
 <body class="mediawiki ns-0 ltr page-Matrix_mathematics">
  <div id="globalWrapper">
   <div id="column-content">
    <div id="content"><a id="top" name="top"></a><h1 class="firstHeading">Matrix (mathematics)</h1>
     <div id="bodyContent">
      <h3 id="siteSub"><a href="../../index.htm">2007 Schools Wikipedia Selection</a>. Related subjects: <a href="../index/subject.Mathematics.htm">Mathematics</a></h3>
      <!-- start content -->
      <p>In <a href="../../wp/m/Mathematics.htm" title="Mathematics">mathematics</a>, a <b>matrix</b> (plural <b>matrices</b>) is a rectangular table of <a href="../../wp/n/Number.htm" title="Number">numbers</a> or, more generally, a table consisting of <!--del_lnk--> abstract quantities that can be added and multiplied. Matrices are used to describe <!--del_lnk--> linear equations, keep track of the <!--del_lnk--> coefficients of <!--del_lnk--> linear transformations and to record data that depend on two parameters. Matrices can be added, multiplied, and decomposed in various ways, making them a key concept in <a href="../../wp/l/Linear_algebra.htm" title="Linear algebra">linear algebra</a> and <!--del_lnk--> matrix theory.<p>In this article, the entries of a matrix are <!--del_lnk--> real or <!--del_lnk--> complex numbers unless otherwise noted.<div class="thumb tright">
       <div style="width:245px;"><a class="internal" href="../../images/263/26385.png.htm" title="Organization of a matrix"><img alt="Organization of a matrix" height="220" longdesc="/wiki/Image:Matrix.png" src="../../images/263/26385.png" width="243" /></a><div class="thumbcaption">
         <div class="magnify" style="float:right"><a class="internal" href="../../images/263/26385.png.htm" title="Enlarge"><img alt="Enlarge" height="11" src="../../images/0/1.png" width="15" /></a></div> Organization of a matrix</div>
       </div>
      </div>
      <p>
       <script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script><a id="Definitions_and_notations" name="Definitions_and_notations"></a><h2> <span class="mw-headline">Definitions and notations</span></h2>
      <p>The horizontal lines in a matrix are called <b>rows</b> and the vertical lines are called <b>columns</b>. A matrix with <i>m</i> rows and <i>n</i> columns is called an <i>m</i>-by-<i>n</i> matrix (written <i>m</i>&times;<i>n</i>) and <i>m</i> and <i>n</i> are called its <b>dimensions</b>. The dimensions of a matrix are always given with the number of rows first, then the number of columns.<p>The entry of a matrix <i>A</i> that lies in the <i>i</i> -th row and the <i>j</i>-th column is called the <i>i,j</i> entry or (<i>i</i>,<i>j</i>)-th entry of <i>A</i>. This is written as <i>A</i><sub>i,j</sub> or <i>A</i>[<i>i,j</i>]. The row is always noted first, then the column.<p>We often write <img alt="A:=(a_{i,j})_{m \times n}" class="tex" src="../../images/327/32739.png" /> to define an <i>m</i> &times; <i>n</i> matrix <i>A</i> with each entry in the matrix <i>A</i>[<i>i,j</i>] called <i>a</i><sub><i>ij</i></sub> for all 1 &le; <i>i</i> &le; <i>m</i> and 1 &le; <i>j</i> &le; <i>n</i>. However, the convention that the indices <i>i</i> and <i>j</i> start at 1 is not universal: some programming languages start at zero, in which case we have 0 &le; <i>i</i> &le; <i>m</i> &minus; 1 and 0 &le; <i>j</i> &le; <i>n</i> &minus; 1.<p>A matrix where one of the dimensions equals one is often called a <i>vector</i>, and interpreted as an element of <!--del_lnk--> real coordinate space. A 1 &times; <i>n</i> matrix (one row and <i>n</i> columns) is called a <!--del_lnk--> row vector, and an <i>m</i> &times; 1 matrix (one column and <i>m</i> rows) is called a <!--del_lnk--> column vector.<p><a id="Example" name="Example"></a><h2> <span class="mw-headline">Example</span></h2>
      <p>The matrix<dl>
       <dd><img alt="A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 1 &amp; 2 &amp; 7 \\ 4&amp;9&amp;2 \\ 6&amp;0&amp;5\end{bmatrix}" class="tex" src="../../images/327/32740.png" /></dl>
      <p>is a 4&times;3 matrix. The element <i>A</i>[2,3] or <i>a</i><sub>2,3</sub> is 7.<p>The matrix<dl>
       <dd><img alt="R = \begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 \end{bmatrix}" class="tex" src="../../images/327/32741.png" /></dl>
      <p>is a 1&times;9 matrix, or 9-element row vector.<p><a id="Adding_and_multiplying_matrices" name="Adding_and_multiplying_matrices"></a><h2> <span class="mw-headline">Adding and multiplying matrices</span></h2>
      <p><a id="Sum" name="Sum"></a><h3> <span class="mw-headline">Sum</span></h3>
      <dl>
       <dd>
      </dl>
      <p>Given <i>m</i>-by-<i>n</i> matrices <i>A</i> and <i>B</i>, their <b>sum</b> <i>A + B</i> is the <i>m</i>-by-<i>n</i> matrix computed by adding corresponding elements (i.e. (<i>A + B</i>)[<i>i, j</i>] = <i>A</i>[<i>i, j</i>] + <i>B</i>[<i>i,&nbsp;j</i>] ). For example:<dl>
       <dd><img alt="\begin{bmatrix}     1 &amp; 3 &amp; 2 \\     1 &amp; 0 &amp; 0 \\     1 &amp; 2 &amp; 2   \end{bmatrix} +   \begin{bmatrix}     0 &amp; 0 &amp; 5 \\     7 &amp; 5 &amp; 0 \\     2 &amp; 1 &amp; 1   \end{bmatrix} =   \begin{bmatrix}     1+0 &amp; 3+0 &amp; 2+5 \\     1+7 &amp; 0+5 &amp; 0+0 \\     1+2 &amp; 2+1 &amp; 2+1   \end{bmatrix} =   \begin{bmatrix}     1 &amp; 3 &amp; 7 \\     8 &amp; 5 &amp; 0 \\     3 &amp; 3 &amp; 3   \end{bmatrix}" class="tex" src="../../images/327/32742.png" /></dl>
      <p>Another, much less often used notion of matrix addition is the <!--del_lnk--> direct sum.<p><a id="Scalar_multiplication" name="Scalar_multiplication"></a><h3> <span class="mw-headline">Scalar multiplication</span></h3>
      <dl>
       <dd>
      </dl>
      <p>Given a matrix <i>A</i> and a number <i>c</i>, the <b><!--del_lnk--> scalar multiplication</b> <i>cA</i> is computed by multiplying the <!--del_lnk--> scalar <i>c</i> by every element of <i>A</i> (i.e. (<i>cA</i>)[<i>i</i>, <i>j</i>] = <i>cA</i>[<i>i</i>, <i>j</i>] ). For example:<dl>
       <dd><img alt="2   \begin{bmatrix}     1 &amp; 8 &amp; -3 \\     4 &amp; -2 &amp; 5   \end{bmatrix} =   \begin{bmatrix}     2\times 1 &amp; 2\times 8 &amp; 2\times -3 \\     2\times 4 &amp; 2\times -2 &amp; 2\times 5   \end{bmatrix} =   \begin{bmatrix}     2 &amp; 16 &amp; -6 \\     8 &amp; -4 &amp; 10   \end{bmatrix}" class="tex" src="../../images/327/32743.png" /></dl>
      <p><a id="Matrix_multiplication" name="Matrix_multiplication"></a><h3> <span class="mw-headline">Matrix multiplication</span></h3>
      <dl>
       <dd>
      </dl>
      <p><b>Multiplication</b> of two matrices is well-defined only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If <i>A</i> is an <i>m</i>-by-<i>n</i> matrix and <i>B</i> is an <i>n</i>-by-<i>p</i> matrix, then their <b>matrix product</b> <i>AB</i> is the <i>m</i>-by-<i>p</i> matrix (<i>m</i> rows, <i>p</i> columns) given by:<dl>
       <dd><img alt="(AB)[i,j] = A[i,1]  B[1,j] + A[i,2]  B[2,j] + ... + A[i,n]  B[n,j] \!\" class="tex" src="../../images/327/32744.png" /></dl>
      <p>for each pair <i>i</i> and <i>j</i>.<p>For example:<dl>
       <dd><img alt="\begin{bmatrix}     1 &amp; 0 &amp; 2 \\     -1 &amp; 3 &amp; 1 \\   \end{bmatrix} \times   \begin{bmatrix}     3 &amp; 1 \\     2 &amp; 1 \\     1 &amp; 0   \end{bmatrix} =   \begin{bmatrix}      (1 \times 3  +  0 \times 2  +  2 \times 1) &amp; (1 \times 1   +   0 \times 1   +   2 \times 0) \\     (-1 \times 3  +  3 \times 2  +  1 \times 1) &amp; (-1 \times 1   +   3 \times 1   +   1 \times 0) \\   \end{bmatrix} =    \begin{bmatrix}     5 &amp; 1 \\     4 &amp; 2 \\   \end{bmatrix}" class="tex" src="../../images/327/32745.png" /></dl>
      <p>These two operations turn the set M(<i>m</i>, <i>n</i>, <b>R</b>) of all <i>m</i>-by-<i>n</i> matrices with real entries into a real <!--del_lnk--> vector space of dimension <i>mn</i>.<p>Matrix multiplication has the following properties:<ul>
       <li>(<i>AB</i>)<i>C</i> = <i>A</i>(<i>BC</i>) for all <i>k</i>-by-<i>m</i> matrices <i>A</i>, <i>m</i>-by-<i>n</i> matrices <i>B</i> and <i>n</i>-by-<i>p</i> matrices <i>C</i> (&quot;associativity&quot;).<li>(<i>A + B</i>)<i>C</i> = <i>AC</i> + <i>BC</i> for all <i>m</i>-by-<i>n</i> matrices <i>A</i> and <i>B</i> and <i>n</i>-by-<i>k</i> matrices <i>C</i> (&quot;right distributivity&quot;).<li><i>C</i>(<i>A + B</i>) = <i>CA</i> + <i>CB</i> for all <i>m</i>-by-<i>n</i> matrices <i>A</i> and <i>B</i> and <i>k</i>-by-<i>m</i> matrices <i>C</i> (&quot;left distributivity&quot;).</ul>
      <p>It is important to note that <!--del_lnk--> commutativity does <i>not</i> generally hold; that is, given matrices <i>A</i> and <i>B</i> and their product defined, then generally <i>AB</i> &ne; <i>BA</i>.<p><a id="Linear_transformations.2C_ranks_and_transpose" name="Linear_transformations.2C_ranks_and_transpose"></a><h2> <span class="mw-headline">Linear transformations, ranks and transpose</span></h2>
      <dl>
       <dd>
      </dl>
      <p>Matrices can conveniently represent <!--del_lnk--> linear transformations because matrix multiplication neatly corresponds to the composition of maps, as will be described next. This same property makes them powerful data structures in high-level programming languages.<p>Here and in the sequel we identify <b>R</b><sup><i>n</i></sup> with the set of &quot;columns&quot; or <i>n</i>-by-1 matrices. For every linear map <i>f</i>&nbsp;: <b>R</b><sup><i>n</i></sup> &rarr; <b>R</b><sup><i>m</i></sup> there exists a unique <i>m</i>-by-<i>n</i> matrix <i>A</i> such that <i>f</i>(<i>x</i>) = <i>Ax</i> for all <i>x</i> in <b>R</b><sup><i>n</i></sup>. We say that the matrix <i>A</i> &quot;represents&quot; the linear map <i>f</i>. Now if the <i>k</i>-by-<i>m</i> matrix <i>B</i> represents another linear map <i>g</i>&nbsp;: <b>R</b><sup><i>m</i></sup> &rarr; <b>R</b><sup><i>k</i></sup>, then the linear map <i>g</i> o <i>f</i> is represented by <i>BA</i>. This follows from the above-mentioned associativity of matrix multiplication.<p>More generally, a linear map from an <i>n</i>-dimensional vector space to an <i>m</i>-dimensional vector space is represented by an <i>m</i>-by-<i>n</i> matrix, provided that <!--del_lnk--> bases have been chosen for each.<p>The <!--del_lnk--> rank of a matrix <i>A</i> is the <!--del_lnk--> dimension of the <!--del_lnk--> image of the linear map represented by <i>A</i>; this is the same as the dimension of the space generated by the rows of <i>A</i>, and also the same as the dimension of the space generated by the columns of <i>A</i>.<p>The <!--del_lnk--> transpose of an <i>m</i>-by-<i>n</i> matrix <i>A</i> is the <i>n</i>-by-<i>m</i> matrix <i>A</i><sup>tr</sup> (also sometimes written as <i>A</i><sup>T</sup> or <sup>t</sup><i>A</i>) formed by turning rows into columns and columns into rows, i.e. <i>A</i><sup>tr</sup>[<i>i</i>, <i>j</i>] = <i>A</i>[<i>j</i>, <i>i</i>] for all indices <i>i</i> and <i>j</i>. If <i>A</i> describes a linear map with respect to two bases, then the matrix <i>A</i><sup>tr</sup> describes the transpose of the linear map with respect to the dual bases, see <!--del_lnk--> dual space.<p>We have (<i>A + B</i>)<sup>tr</sup> = <i>A</i><sup>tr</sup> + <i>B</i><sup>tr</sup> and (<i>AB</i>)<sup>tr</sup> = <i>B</i><sup>tr</sup> <i>A</i><sup>tr</sup>.<p><a id="Square_matrices_and_related_definitions" name="Square_matrices_and_related_definitions"></a><h2> <span class="mw-headline">Square matrices and related definitions</span></h2>
      <p>A <b>square matrix</b> is a matrix which has the same number of rows and columns. The set of all square <i>n</i>-by-<i>n</i> matrices, together with matrix addition and matrix multiplication is a <!--del_lnk--> ring. Unless <i>n</i> = 1, this ring is not <!--del_lnk--> commutative.<p>M(<i>n</i>, <b>R</b>), the ring of real square matrices, is a real unitary <!--del_lnk--> associative algebra. M(<i>n</i>, <b>C</b>), the ring of complex square matrices, is a complex associative algebra.<p>The <b>unit matrix</b> or <b><!--del_lnk--> identity matrix</b> <i>I<sub>n</sub></i>, with elements on the <!--del_lnk--> main diagonal set to 1 and all other elements set to 0, satisfies <i>MI<sub>n</sub>=M</i> and <i>I<sub>n</sub>N=N</i> for any <i>m</i>-by-<i>n</i> matrix <i>M</i> and <i>n</i>-by-<i>k</i> matrix <i>N</i>. For example, if <i>n</i> = 3:<dl>
       <dd><img alt="I_3 =   \begin{bmatrix}     1 &amp; 0 &amp; 0 \\     0 &amp; 1 &amp; 0 \\     0 &amp; 0 &amp; 1   \end{bmatrix}" class="tex" src="../../images/327/32746.png" /></dl>
      <p>The identity matrix is the identity element in the ring of square matrices.<p>Invertible elements in this ring are called <b><!--del_lnk--> invertible matrices</b> or <b>non-singular matrices</b>. An <i>n</i> by <i>n</i> matrix <i>A</i> is invertible if and only if there exists a matrix <i>B</i> such that<dl>
       <dd><i>AB</i> = I<sub><i>n</i></sub> ( = <i>BA</i>).</dl>
      <p>In this case, <i>B</i> is the <b><!--del_lnk--> inverse matrix</b> of <i>A</i>, denoted by <i>A</i><sup>&minus;1</sup>. The set of all invertible <i>n</i>-by-<i>n</i> matrices forms a <a href="../../wp/g/Group_%2528mathematics%2529.htm" title="Group (mathematics)">group</a> (specifically a <!--del_lnk--> Lie group) under matrix multiplication, the <!--del_lnk--> general linear group.<p>If &lambda; is a number and <b>v</b> is a non-zero vector such that <i>A</i><b>v</b> = &lambda;<b>v</b>, then we call <b>v</b> an <!--del_lnk--> eigenvector of <i>A</i> and &lambda; the associated <!--del_lnk--> eigenvalue. (Eigen means &quot;own&quot; in German.) The number &lambda; is an eigenvalue of <i>A</i> if and only if <i>A</i>&minus;&lambda;<i>I</i><sub><i>n</i></sub> is not invertible, which happens if and only if <i>p</i><sub><i>A</i></sub>(&lambda;) = 0. Here <i>p</i><sub><i>A</i></sub>(<i>x</i>) is the <!--del_lnk--> characteristic polynomial of <i>A</i>. This is a <!--del_lnk--> polynomial of degree <i>n</i> and has therefore <i>n</i> complex roots (counting multiple roots according to their multiplicity). In this sense, every square matrix has <i>n</i> complex eigenvalues.<p>The <!--del_lnk--> determinant of a square matrix <i>A</i> is the product of its <i>n</i> eigenvalues, but it can also be defined by the <i><!--del_lnk--> Leibniz formula</i>. Invertible matrices are precisely those matrices with nonzero determinant.<p>The <!--del_lnk--> Gaussian elimination algorithm is of central importance: it can be used to compute determinants, ranks and inverses of matrices and to solve <!--del_lnk--> systems of linear equations.<p>The <!--del_lnk--> trace of a <!--del_lnk--> square matrix is the sum of its diagonal entries, which equals the sum of its <i>n</i> eigenvalues.<p><!--del_lnk--> Matrix exponential is defined for square matrices, using <!--del_lnk--> power series.<p><a id="Special_types_of_matrices" name="Special_types_of_matrices"></a><h2> <span class="mw-headline">Special types of matrices</span></h2>
      <p>In many areas in mathematics, matrices with certain structure arise. A few important examples are<ul>
       <li><!--del_lnk--> Symmetric matrices are such that elements symmetric about the <i>main diagonal</i> (from the upper left to the lower right) are equal, that is, a<sub>i,j</sub>=a<sub>j,i</sub>.<li><!--del_lnk--> Skew-symmetric matrices are such that elements symmetric about the <i>main diagonal</i> are the negative of each other, that is, a<sub>i,j</sub>= - a<sub>j,i</sub>. In a skew-symmetric matrix, all diagonal elements are zero, that is, a<sub>i,i</sub>=0.<li><!--del_lnk--> Hermitian (or self-adjoint) matrices are such that elements symmetric about the diagonal are each others <!--del_lnk--> complex conjugates, that is, a<sub>i,j</sub>=a<sup>*</sup><sub>j,i</sub>, where the superscript &#39;*&#39; signifies complex conjugation.<li><!--del_lnk--> Toeplitz matrices have common elements on their diagonals, that is, a<sub>i,j</sub>=a<sub>i+1,j+1</sub>.<li><!--del_lnk--> Stochastic matrices are square matrices whose columns are <!--del_lnk--> probability vectors; they are used to define <!--del_lnk--> Markov chains.</ul>
      <p>For a more extensive list see <!--del_lnk--> list of matrices.<p><a id="Matrices_in_abstract_algebra" name="Matrices_in_abstract_algebra"></a><h2> <span class="mw-headline">Matrices in abstract algebra</span></h2>
      <p>If we start with a <!--del_lnk--> ring <i>R</i>, we can consider the set M(<i>m</i>,<i>n</i>, <i>R</i>) of all <i>m</i> by <i>n</i> matrices with entries in <i>R</i>. Addition and multiplication of these matrices can be defined as in the case of real or complex matrices (see <a href="#Adding_and_multiplying_matrices" title="">above</a>). The set M(<i>n</i>, <i>R</i>) of all square <i>n</i> by <i>n</i> matrices over <i>R</i> is a ring in its own right, isomorphic to the <!--del_lnk--> endomorphism ring of the left <i>R</i>-<!--del_lnk--> module <i>R</i><sup><i>n</i></sup>.<p>Similarly, if the entries are taken from a <!--del_lnk--> semiring <i>S</i>, matrix addition and multiplication can still be defined as usual. The set of all square <i>n</i>&times;<i>n</i> matrices over <i>S</i> is itself a semiring. Note that fast matrix multiplication algorithms such as the <!--del_lnk--> Strassen algorithm generally only apply to matrices over rings and will not work for matrices over semirings that are not rings.<p>If <i>R</i> is a commutative ring, then M(<i>n</i>, <i>R</i>) is a unitary <!--del_lnk--> associative algebra over <i>R</i>. It is then also meaningful to define the <!--del_lnk--> determinant of square matrices using the <i><!--del_lnk--> Leibniz formula</i>; a matrix is invertible if and only if its determinant is invertible in <i>R</i>.<p>All statements mentioned in this article for real or complex matrices remain correct for matrices over an arbitrary <!--del_lnk--> field.<p>Matrices over a <!--del_lnk--> polynomial ring are important in the study of <!--del_lnk--> control theory.<p><a id="History" name="History"></a><h2> <span class="mw-headline">History</span></h2>
      <p>The study of matrices is quite old. <!--del_lnk--> Latin squares and <!--del_lnk--> magic squares have been studied since prehistoric times.<p>Matrices have a long history of application in solving <!--del_lnk--> linear equations. <a href="../../wp/g/Gottfried_Leibniz.htm" title="Gottfried Leibniz">Leibniz</a>, one of the two founders of calculus, developed the theory of <!--del_lnk--> determinants in <!--del_lnk--> 1693. <!--del_lnk--> Cramer developed the theory further, presenting <!--del_lnk--> Cramer&#39;s rule in <!--del_lnk--> 1750. <a href="../../wp/c/Carl_Friedrich_Gauss.htm" title="Carl Friedrich Gauss">Carl Friedrich Gauss</a> and <!--del_lnk--> Wilhelm Jordan developed <!--del_lnk--> Gauss-Jordan elimination in the 1800s.<p>The term &quot;matrix&quot; was first coined in <!--del_lnk--> 1848 by <!--del_lnk--> J. J. Sylvester. <!--del_lnk--> Cayley, <!--del_lnk--> Hamilton, <!--del_lnk--> Grassmann, <!--del_lnk--> Frobenius and <a href="../../wp/j/John_von_Neumann.htm" title="John von Neumann">von Neumann</a> are among the famous mathematicians who have worked on matrix theory.<p><!--del_lnk--> Olga Taussky Todd (1906-1995) started to use matrix theory when investigating an aerodynamic phenomenon called <!--del_lnk--> fluttering or <!--del_lnk--> aeroelasticity, during <a href="../../wp/w/World_War_II.htm" title="World War II">WWII</a>.<p><a id="Applications" name="Applications"></a><h2> <span class="mw-headline">Applications</span></h2>
      <p><a id="Transportation" name="Transportation"></a><h3> <span class="mw-headline">Transportation</span></h3>
      <p>If one is given a list of cities (or destinations, nodes, etc.) and is told that there are flights (or roads, connections, etc.) from city <i>a</i> to city <i>b</i>, then one can build a square matrix with the cities indexing each side of the matrix. Each entry <i>M</i><sub><i>a</i>,<i>b</i></sub> is set to 1 if there is a connection from <i>a</i> to <i>b</i>; it is 0 otherwise. If there is a reverse connection, going from <i>b</i> to <i>a</i>, then also <i>M</i><sub><i>b</i>,<i>a</i></sub> = 1. In many instances the connection <i>a</i> to <i>b</i> might not be bidirectional, i.e. <i>M</i><sub><i>a</i>,<i>b</i></sub> = 1 does not necessarily imply that <i>M</i><sub><i>b</i>,<i>a</i></sub> = 1.<p>By multiplying the matrix <i>M</i> by itself one obtains <i>M</i><sup>2</sup>. The matrix <i>M</i><sup>2</sup> will indicate if you can go from <i>a</i> to <i>b</i> via a third city. If (<i>M</i><sup>2</sup>)<sub><i>a</i>,<i>b</i></sub> = 1, then there exists one third city <i>c</i> which acts as a layover, that is, you can go from <i>a</i> to <i>c</i> and then from <i>c</i> to <i>b</i>. If (<i>M</i><sup>2</sup>)<sub><i>a</i>,<i>b</i></sub> = <i>n</i>, then there are <i>n</i> such layovers.<p><a id="Encryption" name="Encryption"></a><h3> <span class="mw-headline">Encryption</span></h3>
      <p><i>See <!--del_lnk--> Matrix encryption</i><p>Matrices can be used to encrypt numerical data. Encryption is done by multiplying the data matrix with a key matrix. Decryption is done simply by multiplying the encrypted matrix with the inverse of the key.<p><a id="Computer_Graphics" name="Computer_Graphics"></a><h3> <span class="mw-headline">Computer Graphics</span></h3>
      <p>4x4 transformation matrices are commonly used in computer graphics. The upper left 3x3 portion of a transformation matrix is composed of the new X, Y, and Z axes of the post-transformation coordinate space.<p><a id="Further_reading" name="Further_reading"></a><div class="printfooter"> Retrieved from &quot;<!--del_lnk--> http://en.wikipedia.org/wiki/Matrix_%28mathematics%29&quot;</div>
      <!-- end content -->
      <div class="visualClear">
      </div>
     </div>
    </div>
   </div>
   <!-- end of the left (by default at least) column -->
   <div class="visualClear">
   </div>
   <div id="footer">
    <div class="center"> This reference article is mainly selected from the English Wikipedia with only minor checks and changes (see www.wikipedia.org for details of authors and sources) and is available under the <nobr><a href="../../wp/w/Wikipedia_Text_of_the_GNU_Free_Documentation_License.htm">GNU Free Documentation License</a></nobr>. See also our <b><a href="../../disclaimer.htm">Disclaimer</a></b>. </div>
   </div>
   <script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
  </div>
  
 </body>
</html>
