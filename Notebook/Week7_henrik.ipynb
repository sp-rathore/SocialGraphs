{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7\n",
    "<hr>\n",
    "### Processing real text (from out on the inter-webs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from urllib import urlopen\n",
    "import urllib2\n",
    "import string\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.12__\n",
    "\n",
    "☼ Write a for loop to print out the characters of a string, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      " \n",
      "s\n",
      "t\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "string = \"This is a string.\"\n",
    "for i in string:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 3.6 __\n",
    ">☼ Describe the class of strings matched by the following regular expressions.\n",
    ">\n",
    ">[a-zA-Z]+\n",
    ">\n",
    ">[A-Z][a-z]*\n",
    ">\n",
    ">p[aeiou]{,2}t\n",
    ">\n",
    ">\\d+(\\.\\d+)?\n",
    ">\n",
    ">([^aeiou][aeiou][^aeiou])*\n",
    ">\n",
    ">\\w+|[^\\w\\s]+\n",
    "\n",
    ">Test your answers using nltk.re_show()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[a-zA-Z]+ Will match one or more of the set of chars.\n",
    "\n",
    "[A-Z][a-z]* Will match one of the uppercase char and zero or more of lowercase chars.\n",
    "p[aeiou]{,2}t take a word starting with p after that matching with a char of the set 'aeiou' with no more than 2 repeats and end with t.\n",
    "\n",
    "\\d+(.\\d+)? Will match with something that start with atleast one digit thereafter it will match with zero or more occurrences of any char followed by atleast one digit. The paentheeses scope the match.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.30__\n",
    ">◑ Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('Randomtext.txt')\n",
    "raw = f.read()\n",
    "tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Admiration', 'we', 'surrounded', 'possession', 'frequently', 'he', '.', 'Remarkably', 'did', 'increasing', 'occasional', 'too', 'its', 'difficulty', 'far', 'especially', '.', 'Known', 'tiled', 'but', 'sorry', 'joy', 'balls', '.', 'Bed', 'sudden', 'manner', 'indeed', 'fat', 'now', 'feebly', '.', 'Face', 'do', 'with', 'in', 'need', 'of', 'wife', 'paid', 'that', 'be', '.', 'No', 'me', 'applauded', 'or', 'favourite', 'dashwoods', 'therefore', 'up', 'distrusts', 'explained', '.']\n"
     ]
    }
   ],
   "source": [
    "print tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admir admir\n",
      "frequent frequ\n",
      "Remark remark\n",
      "occasion occas\n",
      "difficulti difficul\n",
      "especi espec\n",
      "Known known\n",
      "tile til\n",
      "sorri sorry\n",
      "ball bal\n",
      "Bed bed\n",
      "sudden sud\n",
      "manner man\n",
      "inde indee\n",
      "feebli feebl\n",
      "Face fac\n",
      "need nee\n",
      "wife wif\n",
      "No no\n",
      "therefor theref\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "p =[porter.stem(t) for t in tokens]\n",
    "l =[lancaster.stem(t) for t in tokens]\n",
    "for i in range(len(p)):\n",
    "    if p[i] != l[i]:\n",
    "        print p[i],l[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr size=\"30\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words that characterize the branches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Setup. We want to start from a clean version of the philosopher pages with as little wiki-markup as possible. \n",
    "> We needed it earlier to get the links, etc, but now we want a readable version. We can get a fairly nice version directly\n",
    "> from the wikipedia API, simply call `prop=extracts&exlimit=max&explaintext` instead of `prop=revisions` as we did earlier.\n",
    "> This will make the API return the text without links and other markup. \n",
    "> \n",
    "> * Use this method to retrive a nice copy of all philosopher's text. You can, of course, also clean the existing pages using\n",
    "> regular expressions, if you like (but that's probably more work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aestheticians', 'epistemologists', 'ethicists', 'logicians', 'metaphysicians', 'political']\n"
     ]
    }
   ],
   "source": [
    "list_of_philo=[]\n",
    "list_of_list=[]\n",
    "for files in os.listdir(\"./philo\"):\n",
    "    if files.endswith(\".txt\"):\n",
    "        f = io.open(\"./philo/\"+files, 'r', encoding='utf-8')\n",
    "        link = re.findall(r'\\*.*\\[\\[(.+?)\\]\\]?',f.read())\n",
    "        name = []\n",
    "        for k in link:\n",
    "            name += [k.split('|')[0]]\n",
    "        list_of_philo += [files.split('.')[0]]\n",
    "        list_of_list += [name]\n",
    "print list_of_philo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total amount of philosophers is 1013\n"
     ]
    }
   ],
   "source": [
    "set_of_philosophers =set()\n",
    "set_of_dup =set()\n",
    "\n",
    "for i in range(len(list_of_list)):\n",
    "    for j in list_of_list[i]:\n",
    "        x = len(set_of_philosophers)\n",
    "        set_of_philosophers.add(j)\n",
    "        if x == len(set_of_philosophers):\n",
    "            set_of_dup.add(j)\n",
    "            \n",
    "print \"The total amount of philosophers is %s\" %len(set_of_philosophers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert set to list\n",
    "list_of_philosophers = list(set_of_philosophers)\n",
    "\n",
    "baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "action = \"action=query\"\n",
    "content = \"prop=extracts&exlimit=max&explaintext&rvprop=content\"\n",
    "dataformat = \"format=json\" \n",
    "for i in range(len(list_of_philosophers)):\n",
    "    title = \"titles=%s\" %list_of_philosophers[i].replace(\" \", \"_\").encode('utf-8')\n",
    "    query= '%s%s&%s&%s&%s' %(baseurl,action,title,content,dataformat)\n",
    "    wikireponse = urllib2.urlopen(query)\n",
    "    wikisource = wikireponse.read()\n",
    "    wikijson = json.loads(wikisource)\n",
    "    filename = \"./json/%s.txt\" %list_of_philosophers[i].replace(\" \", \"_\")\n",
    "    with io.open(filename, 'w',encoding='utf-8') as outfile:\n",
    "        outfile.write(unicode(json.dumps(wikijson, ensure_ascii=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_philosophers = list(set_of_philosophers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":nl:Martinus Dorpius\n",
      "Jaoudat Khouri\n",
      "Andreas Linder\n",
      "T. H. Irwin\n",
      "Graeme Forbes\n",
      "Suzy Kassem\n",
      "Cameron Breen\n",
      "Sam Murgatroyd\n",
      "Tomek Pietkiewicz\n",
      "Anthony de Jasay\n",
      "Luca Vercelloni\n",
      "Charles Blattberg\n"
     ]
    }
   ],
   "source": [
    "for philosoherps in list_of_philosophers:\n",
    "    filename = philosoherps.replace(' ', '_')\n",
    "    with io.open('./json/'+filename+\".txt\",'r',encoding='utf-8') as files:\n",
    "        data = json.load(files)\n",
    "        try:\n",
    "            x = data['query']['pages'].keys()[0]\n",
    "            y = data['query']['pages'][x]['extract']\n",
    "            filenames = \"./json_cleaned/%s.txt\" %filename\n",
    "            with io.open(filenames, 'w',encoding='utf-8') as outfile:\n",
    "                outfile.write(unicode(json.dumps(y, ensure_ascii=False)))\n",
    "        except KeyError:\n",
    "            print philosoherps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * First, check out [the wikipedia page for TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). Explain in your own words the point of TF-IDF. \n",
    ">   * What does TF stand for? \n",
    ">   * What does IDF stand for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TF_:\n",
    "It stand for term frequency and it is self explainatory.\n",
    "\n",
    "_IDF_:\n",
    "Stands for _inverse document frequency_ and its counter the weigth of words that occurs a lot ('the' 'is' etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Since we want to find out which words are important for each _branch_, so we're going to create six large documents, one per branch of philosophy. Tokenize the pages, and combine the tokens into one long list per branch. Remember the bullets below for success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aes_tokens = []\n",
    "epi_tokens = []\n",
    "eth_tokens = []\n",
    "log_tokens = []\n",
    "met_tokens = []\n",
    "pol_tokens = []\n",
    "\n",
    "for files in os.listdir(\"./json_cleaned\"):\n",
    "    if files.endswith(\".txt\"):\n",
    "        philo = files[:-4]\n",
    "        philo = philo.replace('_', ' ')\n",
    "        f = io.open(\"./json_cleaned/\"+files, 'r', encoding='utf-8')\n",
    "        file_content = f.read()\n",
    "        _ = nltk.word_tokenize(file_content)\n",
    "        for i in range(len(list_of_list)):\n",
    "            if philo.decode('utf-8') in list_of_list[i]:\n",
    "                if i is 0:\n",
    "                    aes_tokens.extend(_)\n",
    "                elif i is 1:\n",
    "                    epi_tokens.extend(_)\n",
    "                elif i is 2:\n",
    "                    eth_tokens.extend(_)\n",
    "                elif i is 3:\n",
    "                    log_tokens.extend(_)\n",
    "                elif i is 4:\n",
    "                    met_tokens.extend(_)\n",
    "                elif i is 5:\n",
    "                    pol_tokens.extend(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419577 344090 848613 426068 344165 886657\n"
     ]
    }
   ],
   "source": [
    "print len(aes_tokens),len(epi_tokens),len(eth_tokens), len(log_tokens), len(met_tokens), len(pol_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">   * If you dont' know what _tokenization_ means, go back and read Chapter 3 again. This advice is valid for every cleaning step below.\n",
    ">    * Exclude philosopher names (since we're interested in the words, not the names).\n",
    ">    * Exclude punctuation.\n",
    ">    * Exclude stop words (if you don't know what stop words are, go back and read NLPP1e again).\n",
    ">    * Exclude numbers (since they're difficult to interpret in the word cloud).\n",
    ">    * Set everything to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190622\n",
      "156650\n",
      "390308\n",
      "195306\n",
      "154515\n",
      "415620\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "regex = re.compile('[^\\W\\d_]+$', re.UNICODE)\n",
    "list_of_excep = stopwords+ [x.lower() for x in set_of_philosophers]\n",
    "ii = []\n",
    "for item in [aes_tokens,epi_tokens,eth_tokens,log_tokens,met_tokens,pol_tokens]:\n",
    "    aa = [m.group(0) for w in item for m in [regex.search(w.lower())] if m]\n",
    "    #bb = [w for w in aa if w not in set_of_philosophers] #Remove philos\n",
    "    ii += [[w.lower() for w in aa if w not in list_of_excep]] #Remove stopwords and make lowercase\n",
    "    \n",
    "    \n",
    "for i in range(len(ii)):\n",
    "    print len(ii[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Now, we're ready to calculate the TF for each word. Use the method of your choice to find the top 5 terms within each branch. \n",
    ">   * Describe similarities and differences between the branches.\n",
    ">   * Why aren't the TFs not necessarily a good description of the branches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'philosophy', 1069), (u'one', 899), (u'university', 887), (u'also', 765), (u'work', 753)]\n",
      "[(u'philosophy', 1099), (u'university', 828), (u'one', 789), (u'also', 714), (u'n', 610)]\n",
      "[(u'university', 2011), (u'philosophy', 1912), (u'one', 1738), (u'also', 1650), (u'new', 1320)]\n",
      "[(u'n', 2957), (u'logic', 1743), (u'university', 1440), (u'philosophy', 1232), (u'also', 899)]\n",
      "[(u'n', 1414), (u'philosophy', 1380), (u'university', 927), (u'one', 786), (u'also', 622)]\n",
      "[(u'university', 2069), (u'philosophy', 1870), (u'political', 1746), (u'also', 1627), (u'new', 1533)]\n"
     ]
    }
   ],
   "source": [
    "for i in ii:\n",
    "    fdist = nltk.FreqDist(i)\n",
    "    print fdist.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Next, we calculate IDF for every word. \n",
    ">   * What base logarithm did you use? Is that important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.405465108108\n"
     ]
    }
   ],
   "source": [
    "mytexts = nltk.TextCollection(ii)\n",
    "print mytexts.idf('prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1233.03034282 seconds ---\n",
      "0.405465108108\n"
     ]
    }
   ],
   "source": [
    "#start_time = time.time()\n",
    "#tokens = set(x for x in i for i in ii)\n",
    "#dictIdf = {x: mytexts.idf(x) for x in tokens}\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#print dictIdf[\"prof\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findIDF(word):\n",
    "    counter = 0\n",
    "    idf = 0\n",
    "    for i in ii:\n",
    "        if word in i:\n",
    "            counter = counter + 1\n",
    "    idf = math.log(6/counter)\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-fd9a1af8f851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mbags_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0midf_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfindIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbags_word\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midf_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-fd9a1af8f851>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m((word,))\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mbags_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0midf_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfindIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbags_word\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midf_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-6c56beb173d7>\u001b[0m in \u001b[0;36mfindIDF\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bags_word = set()\n",
    "for branch in ii:\n",
    "    for word in branch:\n",
    "        bags_word.add(word)\n",
    "\n",
    "idf_dict = {word: findIDF(word) for word in bags_word}\n",
    "print len(idf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk uses a log witht the base of _e_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> * We're ready to calculate TF-IDF. Do that for each branch. \n",
    ">   * List the 10 top words for each branch.\n",
    ">   * Are these 10 words more descriptive of the branch? If yes, what is it about IDF that makes the words more informative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-8b8d86770eab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfdist1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mtemp_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfindIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mlost_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-6c56beb173d7>\u001b[0m in \u001b[0;36mfindIDF\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_of_tfidf = []\n",
    "lost_words = []\n",
    "counter = 0\n",
    "for i in ii:\n",
    "    fdist = nltk.FreqDist(i)\n",
    "    fdist1 = fdist.most_common()\n",
    "    temp_list= []\n",
    "    for element in fdist1:\n",
    "        try:\n",
    "            temp_list.append((element[0],element[1]*findIDF(element[0])))\n",
    "        except KeyError:\n",
    "            lost_words.append(element[0])\n",
    "    list_of_tfidf.append(temp_list)\n",
    "    counter = counter +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44572 42031\n"
     ]
    }
   ],
   "source": [
    "print len(lost_words),len(dictIdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
